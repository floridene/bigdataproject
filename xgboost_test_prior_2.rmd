---
title: "xgboost neu"
output: html_notebook
---

# Preparation
Data:
```{r}
#aisles <- read.csv(unz("/home/Vera_Weidmann/Supermarket/00_Data/aisles.csv.zip", "aisles.csv"), stringsAsFactors = FALSE)

#departments <- read.csv(unz("/home/Vera_Weidmann/Supermarket/00_Data/departments.csv.zip", "departments.csv"), stringsAsFactors = FALSE)

orderp <- read.csv(unz("/home/Vera_Weidmann/Supermarket/00_Data/order_products__prior.csv.zip", "order_products__prior.csv"), stringsAsFactors = FALSE)

ordert <- read.csv(unz("/home/Vera_Weidmann/Supermarket/00_Data/order_products__train.csv.zip", "order_products__train.csv"), stringsAsFactors = FALSE)

orders <- read.csv(unz("/home/Vera_Weidmann/Supermarket/00_Data/orders.csv.zip", "orders.csv"), stringsAsFactors = FALSE)

products <- read.csv(unz("/home/Vera_Weidmann/Supermarket/00_Data/products.csv.zip", "products.csv"), stringsAsFactors = FALSE)
```

Libraries: 
```{r, echo = F}
library(data.table)
library(dplyr)
library(tidyr)
library(xgboost)
```

Functions:
```{r}
#function for viewing data for big datatables
vh <- function(x){
  View(head(x, 20))
}
```


# Important facts about data: 
* "order prior" contains historical data about train and test users.
* "order train" just contains the last basket from train users (can be used as valdiation?)
* "orders"contains all different users: prior, train and the test users. Task: Predict the next basket for the test users!

# Reshape data
```{r}
#convert into factors:
aisles$aisle <- as.factor(aisles$aisle)
departments$department <- as.factor(departments$department)
orders$eval_set <- as.factor(orders$eval_set)
products$product_name <- as.factor(products$product_name)
#user_id and order_id?

#join aisle-names and department-names with products
#products <- products %>% 
#  inner_join(aisles, by = "aisle_id") %>% inner_join(departments, by = "department_id") %>% 
#  select(-aisle_id, -department_id)
#rm(aisles, departments)

#match user_id to orders train. 
ordert$user_id <- orders$user_id[match(ordert$order_id, orders$order_id)] #order-products train
orderp$user_id <- orders$user_id[match(orderp$order_id, orders$order_id)] #order-products test
vh(orderp)

#orders_products <- orders %>% inner_join(orderp, by = "order_id")

rm(orderp)
gc()

#These are the test users and orders (which are contained by the dataset orders)
testorders <- orders %>% filter(eval_set=="test") %>% select(user_id,order_id)
testusers <- testorders[,1] #75000 users
```

We want to train our model just on the features based on test users. Therefore, we create a subset "prior_test", which contains just the historical shopping behavior from a test user. Training users are excluded. 
```{r}
prior_test <- orders %>% filter(eval_set=="prior" & user_id %in% testusers)
prior_test <- prior_test %>% select(-user_id) %>% inner_join(orderp, by = "order_id")
vh(prior_test)

#first order: NA can be eliminated
#prior_test <- prior_test %>% filter(days_since_prior_order != is.na(days_since_prior_order))
```

# New Product Features:
* Reorder Probability 
* Reorder Times
* Reorder Ratio 

```{r}
prd <- prior_test %>%
  arrange(user_id, order_number, product_id) %>%
  group_by(user_id, product_id) %>%
  mutate(product_time = row_number()) %>%
  ungroup() %>%
  group_by(product_id) %>%
  summarise(
    prod_orders = n(),
    prod_reorders = sum(reordered),
    prod_first_orders = sum(product_time == 1),
    prod_second_orders = sum(product_time == 2),
    prod_third_orders = sum(product_time >= 3)
  )

prd$prod_reorder_probability <- prd$prod_second_orders / prd$prod_first_orders
prd$prod_reorder_times <- 1 + prd$prod_reorders / prd$prod_first_orders
prd$prod_reorder_ratio <- prd$prod_reorders / prd$prod_orders

#join information about aisles and departments:
prd <- prd %>% inner_join(products[,-2], by = "product_id")

#select columns you like to have:
prd <- prd %>% select(-prod_reorders, -prod_first_orders, -prod_second_orders)

#scale nw features
#prd[,c(2:5,7)] <- scale(prd[,c(2:5,7)])

rm(products)
gc()
#vh(prd)
```

# New User Features:
```{r}
users <- prior_test %>%
  group_by(user_id) %>%
  summarise(
    user_total_products = n(),
    user_reorder_ratio = sum(reordered == 1) / sum(order_number > 1),
    user_distinct_products = n_distinct(product_id),
    user_orders = max(order_number),
    user_period = sum(days_since_prior_order, na.rm = T),
    user_mean_days_since_prior = mean(days_since_prior_order, na.rm = T)
  )

users$user_average_basket <- users$user_total_products / users$user_orders

#save user average basket for later
user_average_basket <- users[,c(1,8)]
user_average_basket$user_average_basket <- round(user_average_basket$user_average_basket)

#scale new features:
#users[,c(2:5,7,8)] <- scale(users[,c(2:5,7,8)])

vh(users)
```


# Order features:
```{r}
ord <- prior_test %>%
  group_by(user_id, product_id) %>% 
  summarise(
    up_orders = n(),
    up_first_order = min(order_number),
    up_last_order = max(order_number),
    up_average_cart_position = mean(add_to_cart_order))
```

# Joining newfeatures into prior_test:
```{r}
prior_test <- prior_test %>% 
  inner_join(prd, by = "product_id") %>%
  inner_join(users, by = "user_id") %>%
  arrange(-user_id)

prior_test <- merge(prior_test, ord, x.by = c("user_id", "product_id"), sort = F)

prior_test <- prior_test %>% arrange(-user_id) %>% select(-eval_set, -order_id)

vh(prior_test)
vh(orders)
```


#Prepare test data for prediction:
```{r}
test <- prior_test %>% select(-order_id,-reordered) %>% group_by(user_id,product_id) %>%  filter(order_number==max(order_number)) %>% arrange(product_id,user_id)
test <- test[,-c(3:7)] #elvaset until days since order
test <- test %>% inner_join(orders[,-1] %>% filter(eval_set=="test"), by="user_id")

vh(prior_test)
vh(test)
```

```{r}
prior_test <- prior_test %>% filter(!is.na(days_since_prior_order)) #%>% select(-eval_set,-order_id)
```

```{r}
#save(prior_test, file="prior_test.rda")
#save(test,file="test.rda")
#load("/home/Max_Philipp/bigdataproject/test.rda")
#write.csv(prior_test, file="prior_test.csv")
#write.csv(test,file="test_prior_test.csv")
```

```{r}
rm(orders)
rm(orders_products)
rm(ordert)
rm(prd)
rm(users)
rm(products)
```


```{r}
load("prior_test.rda")

subtrain <- prior_test %>% sample_frac(0.2)
X <- xgb.DMatrix(as.matrix(subtrain %>% select(-reordered)), label = subtrain$reordered)
```

test for best params
```{r}
best_param <- list()
best_seednumber <- 1234
best_logloss <- Inf
best_logloss_index <- 0
best_CVround <- 0



for (iter in 1:20) {
   param <- list(objective = "binary:logistic",
                 eval_metric = "logloss",
                 max_depth = sample(6:12, 1),
                 eta = runif(1, .01, .3),
                 gamma = runif(1, 0.0, 0.2),
                 subsample = runif(1, .6, .9),
                 colsample_bytree = runif(1, .5, .8),
                 min_child_weight = sample(1:40, 1),
                 max_delta_step = sample(1:10, 1)
                 )
   
   ## cv_nround <- c(250, 500, 1000) # We first choose three different numbers of rounds. However, putting early_stopping in place made this step obsolete. Therefere, we were able to get rid of the nested for loop.
   cv_nround <- 100
   cv_nfold <- 5
   seed_number = sample.int(10000, 1)[[1]]
   set.seed(seed_number)
   message("Iteration Round: ", as.character(iter), appendLF = FALSE) ## Check at which iteration we are.
   
   ## for (validator in cv_nround) {
     mdcv <- xgb.cv(data = X,
                  params = param,
                  nfold = cv_nfold,
                  nrounds = cv_nround,
                  nthread = 32,
                  verbose = TRUE,
                  early_stopping_rounds = 20,
                  maximize = FALSE
                  )

   min_logloss <- min(mdcv$evaluation_log$test_logloss_mean)
   min_logloss_index <- which.min(mdcv$evaluation_log$test_logloss_mean)

   if (min_logloss < best_logloss) {
       best_logloss = min_logloss
       best_logloss_index = min_logloss_index
       best_seednumber = seed_number
       best_param = param
       ## best_CVround = mdcv$niter
       
      }
  ## }
}

## Save the best parameters
write.csv(best_param, file = "best_parameters_FinalCV.csv")

```

#fitting the best model

```{r}
subtrain <- prior_test %>% sample_frac(0.3)
X <- xgb.DMatrix(as.matrix(subtrain %>% select(-reordered)), label = subtrain$reordered)

#list from kaggle:
# param <- list(
#   "objective"           = "reg:logistic",
#   "eval_metric"         = "logloss",
#   "eta"                 = 0.1,
#   "max_depth"           = 6,
#   "min_child_weight"    = 10,
#   "gamma"               = 0.70,
#   "subsample"           = 0.76,
#   "colsample_bytree"    = 0.95,
#   "alpha"               = 2e-05,
#   "lambda"              = 10
# )

param <- list(objective = "binary:logistic",
                 eval_metric = "logloss",
                 max_depth = 9,
                 eta = 0.08910454,
                 gamma = 0.1459201,
                 subsample = 0.7306826,
                 colsample_bytree = 0.6887265,
                 min_child_weight = 32,
                 max_delta_step = 8)
                 
cv_nround <- 200
cv_nfold <- 5

mdcv <- xgboost(data = X,
                  params = param,
                  nfold = cv_nfold,
                  nrounds = cv_nround,
                  nthread = 32,
                  verbose = TRUE,
                  early_stopping_rounds = 20,
                  maximize = FALSE
                  )

Y <- xgb.DMatrix(as.matrix(test))
test$reordered <- predict(mdcv, Y)
save(test,file="test_with_preds_xgb1307_blödsinn.rda")
```



ab hier: preds1207 from h2o distributed random forest 50 trees 5fold

```{r}
#user_average_basket$user_average_basket <- round(as.numeric(user_average_basket$user_average_basket))
```

```{r}
#test$reordered <- preds_xgb$reordered #enter predictions here


submission <- test %>% 
  select(-user_average_basket) %>% 
  inner_join(user_average_basket, by="user_id")%>% 
  arrange(user_id,-reordered)%>% 
  group_by(user_id) %>% 
  mutate(index=row_number()) %>% 
  filter(index<=user_average_basket) %>% 
  ungroup() %>% 
  inner_join(testorders,by="user_id") %>% 
  select(order_id,product_id) %>%
  group_by(order_id) %>% 
  summarise(products = paste(product_id, collapse = " "))

write.csv(submission, file = "sub_xgb_notscaled_blödsinn_2.csv", row.names = F)
```

